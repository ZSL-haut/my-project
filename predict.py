"""
çƒŸè‰é«˜å…‰è°±å›¾åƒåˆ†ç±» - å•æ ·æœ¬é¢„æµ‹è„šæœ¬
æ”¯æŒå‘½ä»¤è¡Œå’Œäº¤äº’å¼ä¸¤ç§ä½¿ç”¨æ–¹å¼
"""
import os
import sys
import numpy as np
import torch
import torch.nn as nn
from PIL import Image
import torchvision.transforms as transforms
import torchvision.models as models
import argparse

# å°è¯•å¯¼å…¥ timm
try:
    import timm

    HAS_TIMM = True
except ImportError:
    HAS_TIMM = False
    print("âš ï¸  è­¦å‘Š: timm åº“æœªå®‰è£…ï¼Œéƒ¨åˆ†æ¨¡å‹ä¸å¯ç”¨ï¼ˆEfficientNet, ViT, ConvNeXtï¼‰")
    print("   å¯è¿è¡Œ: pip install timm\n")


# ==================== é…ç½®ç±» ====================
class Config:
    """å…¨å±€é…ç½®"""
    classes = ["å¾®å¸¦é’", "æ‚è‰²", "æŸ æª¬é»„", "æ©˜é»„", "çº¢æ£•", "é’è‰²"]
    image_size = (224, 224)
    max_npy_channels = 6
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    feature_dim = 512

    # æ¨¡å‹æƒé‡è·¯å¾„
    model_weights = {
        'resnet34': './results/result1/ResNet34/best_model_resnet34.pth',
        'resnet50': './results/result1/ResNet50/best_model_resnet50.pth',
        'efficientnet_b0': './results/result1/efficientnet_b0/best_model_efficientnet_b0.pth',
        'vit_base_patch16_224': './results/result1/vit_base_patch16_224/best_model_vit_base_patch16_224.pth',
        'convnext_tiny': './results/result1/convnext_tiny/best_model_convnext_tiny.pth',
    }

    # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆä»…ç”¨äºæ¨¡å‹ç»“æ„åˆå§‹åŒ–ï¼‰
    pretrained_paths = {
        'resnet34': "./pre_models/resnet34.pth",
        'resnet50': "./pre_models/resnet50.pth",
        'efficientnet_b0': "./pre_models/efficientnet_b0.bin",
        'vit_base_patch16_224': "./pre_models/vit_base_patch16_224.bin",
        'convnext_tiny': "./pre_models/convnext_tiny.bin",
    }


# ==================== æ¨¡å‹å®šä¹‰ ====================
class MultimodalModel(nn.Module):
    """å¤šæ¨¡æ€èåˆæ¨¡å‹ï¼ˆå›¾åƒ + é«˜å…‰è°±ï¼‰"""

    def __init__(self, num_classes, backbone_name='resnet34'):
        super().__init__()
        self.backbone_name = backbone_name

        # å›¾åƒç‰¹å¾æå–å™¨
        self.img_backbone, self.feature_dim_img = self._build_image_backbone(backbone_name)

        # é«˜å…‰è°±ç‰¹å¾æå–å™¨ï¼ˆåç§°æ”¹ä¸º npy_feature_extractorï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        self.npy_feature_extractor = nn.Sequential(
            nn.Conv2d(Config.max_npy_channels, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),

            nn.Conv2d(128, Config.feature_dim, kernel_size=3, padding=1),
            nn.BatchNorm2d(Config.feature_dim),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1)
        )

        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim_img + Config.feature_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def _build_image_backbone(self, backbone_name):
        """æ„å»ºå›¾åƒéª¨å¹²ç½‘ç»œ"""
        name = backbone_name.lower()

        if name == 'resnet34':
            model = models.resnet34(weights=None)
            feature_dim = model.fc.in_features
            model.fc = nn.Identity()

        elif name == 'resnet50':
            model = models.resnet50(weights=None)
            feature_dim = model.fc.in_features
            model.fc = nn.Identity()

        elif name == 'efficientnet_b0':
            if not HAS_TIMM:
                raise ImportError("EfficientNet éœ€è¦ timm åº“ï¼Œè¯·è¿è¡Œ: pip install timm")
            model = timm.create_model('efficientnet_b0', pretrained=False)
            feature_dim = model.classifier.in_features
            model.classifier = nn.Identity()

        elif name == 'vit_base_patch16_224':
            if not HAS_TIMM:
                raise ImportError("ViT éœ€è¦ timm åº“ï¼Œè¯·è¿è¡Œ: pip install timm")
            model = timm.create_model('vit_base_patch16_224', pretrained=False)
            feature_dim = model.head.in_features
            model.head = nn.Identity()

        elif name == 'convnext_tiny':
            if not HAS_TIMM:
                raise ImportError("ConvNeXt éœ€è¦ timm åº“ï¼Œè¯·è¿è¡Œ: pip install timm")
            model = timm.create_model('convnext_tiny', pretrained=False)
            feature_dim = model.head.fc.in_features
            model.head.fc = nn.Identity()

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {backbone_name}")

        return model, feature_dim

    def forward(self, x):
        img, npy = x

        # æå–å›¾åƒç‰¹å¾
        img_feat = self.img_backbone(img)
        if img_feat.ndim == 3 and ('vit' in self.backbone_name or 'swin' in self.backbone_name):
            img_feat = img_feat[:, 0]  # å– [CLS] token
        elif img_feat.ndim > 2:
            img_feat = torch.flatten(img_feat, 1)

        # æå–é«˜å…‰è°±ç‰¹å¾
        npy_feat = self.npy_feature_extractor(npy)
        npy_feat = torch.flatten(npy_feat, 1)

        # ç‰¹å¾èåˆ + åˆ†ç±»
        combined = torch.cat([img_feat, npy_feat], dim=1)
        return self.classifier(combined)


# ==================== æ•°æ®é¢„å¤„ç† ====================
def load_and_preprocess_image(img_path):
    """åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ"""
    if not os.path.exists(img_path):
        raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_path}")

    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(Config.image_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    img = Image.open(img_path).convert('RGB')
    img_tensor = transform(img).unsqueeze(0)  # (1, 3, 224, 224)
    return img_tensor


def load_and_preprocess_npy(npy_path):
    """åŠ è½½å¹¶é¢„å¤„ç†é«˜å…‰è°±æ•°æ®"""
    if not os.path.exists(npy_path):
        raise FileNotFoundError(f"NPY æ–‡ä»¶ä¸å­˜åœ¨: {npy_path}")

    # åŠ è½½æ•°æ®
    npy_data = np.load(npy_path).astype(np.float32)

    # ç¡®ä¿æ˜¯ 3D (H, W, C)
    if npy_data.ndim == 2:
        npy_data = np.expand_dims(npy_data, axis=-1)

    # é€šé“æ•°å¯¹é½
    current_channels = npy_data.shape[-1]
    if current_channels < Config.max_npy_channels:
        # é›¶å¡«å……
        padded = np.zeros((*npy_data.shape[:2], Config.max_npy_channels), dtype=np.float32)
        padded[:, :, :current_channels] = npy_data
        npy_data = padded
    elif current_channels > Config.max_npy_channels:
        # æˆªæ–­
        npy_data = npy_data[:, :, :Config.max_npy_channels]

    # Min-Max å½’ä¸€åŒ–ï¼ˆä½¿ç”¨ç™¾åˆ†ä½æ•°é¿å…å¼‚å¸¸å€¼ï¼‰
    min_val = np.percentile(npy_data, 5)
    max_val = np.percentile(npy_data, 95)
    npy_range = max_val - min_val

    if npy_range < 1e-6:
        npy_data = np.full_like(npy_data, 0.5)
    else:
        npy_data = (npy_data - min_val) / npy_range
    npy_data = np.clip(npy_data, 0, 1)

    # è½¬æ¢ä¸º Tensor (C, H, W)
    npy_tensor = torch.from_numpy(npy_data).permute(2, 0, 1).float()

    # Resize
    resize_transform = transforms.Resize(Config.image_size, antialias=True)
    npy_tensor = resize_transform(npy_tensor).unsqueeze(0)  # (1, 6, 224, 224)

    return npy_tensor


# ==================== æ¨¡å‹åŠ è½½ ====================
def load_model(model_name):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # æ£€æŸ¥æ¨¡å‹æƒé‡æ˜¯å¦å­˜åœ¨
    weight_path = Config.model_weights.get(model_name)
    if not weight_path:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹åç§°: {model_name}\nå¯ç”¨æ¨¡å‹: {list(Config.model_weights.keys())}")

    if not os.path.exists(weight_path):
        raise FileNotFoundError(
            f"æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}\n"
            f"è¯·ç¡®ä¿å·²è®­ç»ƒè¯¥æ¨¡å‹å¹¶ä¿å­˜æƒé‡æ–‡ä»¶ã€‚"
        )

    # åˆå§‹åŒ–æ¨¡å‹ç»“æ„
    print(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}...")
    model = MultimodalModel(
        num_classes=len(Config.classes),
        backbone_name=model_name
    )

    # åŠ è½½æƒé‡
    try:
        state_dict = torch.load(weight_path, map_location=Config.device)
        model.load_state_dict(state_dict)
        model.to(Config.device)
        model.eval()
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model
    except Exception as e:
        raise RuntimeError(f"åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")


# ==================== é¢„æµ‹å‡½æ•° ====================
def predict(img_path, npy_path, model_name='resnet34', show_all_probs=True):
    """
    æ‰§è¡Œå•æ ·æœ¬é¢„æµ‹

    Args:
        img_path: å›¾åƒæ–‡ä»¶è·¯å¾„
        npy_path: NPY æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°
        show_all_probs: æ˜¯å¦æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡

    Returns:
        dict: é¢„æµ‹ç»“æœå­—å…¸
    """
    # æ‰“å°æ ‡é¢˜
    print(f"\n{'=' * 100}")
    print(f"ğŸŒ¿ çƒŸè‰é¢œè‰²åˆ†ç±»é¢„æµ‹ç³»ç»Ÿ")
    print(f"{'=' * 100}")
    print(f"ğŸ“· å›¾åƒæ–‡ä»¶: {os.path.basename(img_path)}")
    print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {os.path.basename(npy_path)}")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")
    print(f"ğŸ’» è¿è¡Œè®¾å¤‡: {Config.device}")
    print(f"{'=' * 100}\n")

    # 1. åŠ è½½æ•°æ®
    print("ğŸ“¥ æ­£åœ¨åŠ è½½æ•°æ®...")
    try:
        img_tensor = load_and_preprocess_image(img_path)
        npy_tensor = load_and_preprocess_npy(npy_path)
        print(f"  âœ“ å›¾åƒå°ºå¯¸: {img_tensor.shape}")
        print(f"  âœ“ NPYå°ºå¯¸:  {npy_tensor.shape}")
    except Exception as e:
        print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    # 2. åŠ è½½æ¨¡å‹
    try:
        model = load_model(model_name)
    except Exception as e:
        print(f"  âŒ {e}")
        return None

    # 3. æ‰§è¡Œæ¨ç†
    print(f"\nğŸ”® æ­£åœ¨é¢„æµ‹...")
    with torch.no_grad():
        img_tensor = img_tensor.to(Config.device)
        npy_tensor = npy_tensor.to(Config.device)

        outputs = model((img_tensor, npy_tensor))
        probabilities = torch.softmax(outputs, dim=1)[0]  # (num_classes,)

        predicted_idx = torch.argmax(probabilities).item()
        confidence = probabilities[predicted_idx].item()

    predicted_class = Config.classes[predicted_idx]

    # 4. è¾“å‡ºç»“æœ
    print(f"\n{'=' * 100}")
    print(f"âœ… é¢„æµ‹å®Œæˆ")
    print(f"{'=' * 100}")
    print(f"ğŸ¯ é¢„æµ‹ç±»åˆ«: \033[1;32m{predicted_class}\033[0m")
    print(f"ğŸ“ˆ ç½®ä¿¡åº¦:   \033[1;36m{confidence:.2%}\033[0m")

    # æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«æ¦‚ç‡
    if show_all_probs:
        print(f"\n{'â”€' * 100}")
        print(f"ğŸ“Š æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ:")
        print(f"{'â”€' * 100}")

        # æŒ‰æ¦‚ç‡æ’åº
        probs_with_class = [(Config.classes[i], probabilities[i].item())
                            for i in range(len(Config.classes))]
        probs_with_class.sort(key=lambda x: x[1], reverse=True)

        for rank, (cls, prob) in enumerate(probs_with_class, 1):
            # ç”Ÿæˆè¿›åº¦æ¡
            bar_length = int(prob * 40)
            bar = 'â–ˆ' * bar_length + 'â–‘' * (40 - bar_length)

            # é«˜äº®æ˜¾ç¤ºé¢„æµ‹ç±»åˆ«
            if cls == predicted_class:
                print(f"  {rank}. \033[1;32m{cls:10s}\033[0m â”‚{bar}â”‚ {prob:6.2%}")
            else:
                print(f"  {rank}. {cls:10s} â”‚{bar}â”‚ {prob:6.2%}")

        print(f"{'â”€' * 100}")

    print(f"{'=' * 100}\n")

    # è¿”å›ç»“æœå­—å…¸
    result = {
        'predicted_class': predicted_class,
        'confidence': confidence,
        'probabilities': {Config.classes[i]: probabilities[i].item()
                          for i in range(len(Config.classes))},
        'model_name': model_name
    }

    return result


# ==================== äº¤äº’å¼è¾“å…¥ ====================
def interactive_mode():
    """äº¤äº’å¼è¾“å…¥æ¨¡å¼"""
    print("\n" + "=" * 100)
    print("ğŸŒ¿ çƒŸè‰é¢œè‰²åˆ†ç±»é¢„æµ‹ç³»ç»Ÿ - äº¤äº’å¼æ¨¡å¼")
    print("=" * 100)

    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    print("\nè¯·è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå¯ç›´æ¥æ‹–æ‹½æ–‡ä»¶åˆ°ç»ˆç«¯ï¼‰:")
    img_path = input("ğŸ“· å›¾åƒæ–‡ä»¶è·¯å¾„: ").strip().strip('"').strip("'")
    npy_path = input("ğŸ“Š NPYæ–‡ä»¶è·¯å¾„:  ").strip().strip('"').strip("'")

    # é€‰æ‹©æ¨¡å‹
    print("\nå¯ç”¨æ¨¡å‹:")
    available_models = list(Config.model_weights.keys())
    for i, model in enumerate(available_models, 1):
        print(f"  {i}. {model}")

    model_choice = input(f"\né€‰æ‹©æ¨¡å‹ (1-{len(available_models)}) [é»˜è®¤: 1]: ").strip()
    if not model_choice:
        model_choice = '1'

    try:
        model_idx = int(model_choice) - 1
        model_name = available_models[model_idx]
    except (ValueError, IndexError):
        print("âš ï¸  æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: resnet34")
        model_name = 'resnet34'

    # æ‰§è¡Œé¢„æµ‹
    predict(img_path, npy_path, model_name, show_all_probs=True)


# ==================== å‘½ä»¤è¡Œæ¥å£ ====================
def main():
    parser = argparse.ArgumentParser(
        description='çƒŸè‰é¢œè‰²åˆ†ç±» - å•æ ·æœ¬é¢„æµ‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å‘½ä»¤è¡Œæ¨¡å¼
  python predict.py --img test.jpg --npy test.npy --model resnet34

  # äº¤äº’å¼æ¨¡å¼
  python predict.py

  # Windows è·¯å¾„ç¤ºä¾‹
  python predict.py --img "F:\\data\\test\\å¾®å¸¦é’\\sample.jpg" --npy "F:\\data\\test\\å¾®å¸¦é’\\sample.npy"

å¯ç”¨æ¨¡å‹:
  - resnet34 (é»˜è®¤)
  - resnet50
  - efficientnet_b0 (éœ€è¦ timm)
  - vit_base_patch16_224 (éœ€è¦ timm)
  - convnext_tiny (éœ€è¦ timm)
        """
    )

    parser.add_argument('--img', type=str, help='å›¾åƒæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--npy', type=str, help='NPY æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', type=str, default='resnet34',
                        choices=list(Config.model_weights.keys()),
                        help='æ¨¡å‹åç§° (é»˜è®¤: resnet34)')
    parser.add_argument('--hide-probs', action='store_true',
                        help='ä¸æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ')

    args = parser.parse_args()

    # åˆ¤æ–­è¿è¡Œæ¨¡å¼
    if args.img and args.npy:
        # å‘½ä»¤è¡Œæ¨¡å¼
        predict(
            img_path=args.img,
            npy_path=args.npy,
            model_name=args.model,
            show_all_probs=not args.hide_probs
        )
    else:
        # äº¤äº’å¼æ¨¡å¼
        if args.img or args.npy:
            print("âš ï¸  è­¦å‘Š: éœ€è¦åŒæ—¶æä¾› --img å’Œ --npy å‚æ•°")
            print("   åˆ‡æ¢åˆ°äº¤äº’å¼æ¨¡å¼...\n")
        interactive_mode()


if __name__ == "__main__":
    main()